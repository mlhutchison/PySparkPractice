Practice2

sqoop import -help
sqoop export -help

QUESTION 1
sqoop import \
--connect "jdbc:mysql://localhost/retail_db" \
--username root --password cloudera \
--table products \
--fields-terminated-by ":" \
--lines-terminated-by "|" \
--hive-database default \
--hive-table product_new \
--as-parquetfile

QUESTION 2

Using sqoop export all data from metastore product_new table 
created in last problem statement into products_hive table table in mysql. 
Data Description:
A mysql instance is running on the gateway node.
> Installation : on the cluser node gateway 
> Database name:  retail_db
> Table name: product_hive
> Username: root
> Password: cloudera

Output Requirement:
product_hive  table should contain all product data 
imported from hive table.

sqoop export \
--connect "jdbc:mysql://localhost/retail_db" \
--username root --password cloudera \
--table product_hive \
--hcatalog-table product_new

QUESTION 3 PREP
sqoop import --connect "jdbc:mysql://localhost/retail_db" --password cloudera --username root --table orders --fields-terminated-by "\t" --target-dir /user/cloudera/practice2/problem3/orders
 
sqoop import --connect "jdbc:mysql://localhost/retail_db" --password cloudera --username root --table order_items --fields-terminated-by "\t" --target-dir /user/cloudera/practice2/problem3/order_items
 
sqoop import --connect "jdbc:mysql://localhost/retail_db" --password cloudera --username root --table customers --fields-terminated-by "\t" --target-dir /user/cloudera/practice2/problem3/customers

QUESTION 3
Get all customers who have placed order of amount more than 200.
Input files are tab delimeted files placed at below HDFS location:
/user/cloudera/practice2/problem3/customers
/user/cloudera/practice2/problem3/orders
/user/cloudera/practice2/problem3/order_items

Schema for customers File
Customer_id,customer_fname,customer_lname,
##customer_email,customer_password,customer_street,
##customer_city,customer_state,customer_zipcode
 
Schema for Orders File
Order_id,order_date,order_customer_id,order_status
 
Schema for Order_Items File
Order_item_id,Order_item_order_id,order_item_product_id,
Order_item_quantity,Order_item_subtotal,Order_item_product_price

>> Output should be placed in below HDFS Location
/user/cloudera/practice2/problem3/joinResults

>> Output file should be comma seperated file with 
customer_fname,customer_lname,customer_city,order_amount

//make schemas
case class cust(customer_id:String, fname: String, lname: String, city:String)
case class order(order_id: String, customer_id:String)
case class orderItems(order_id:String,price:Float)
//map dataframes
val custDF = sc.textFile("/user/cloudera/practice2/problem3/customers").map(x=>x.split("\t")).map(c=>cust(c(0),c(1),c(2),c(6))).toDF()
val orderDF = sc.textFile("/user/cloudera/practice2/problem3/orders").map(x=>x.split("\t")).map(o=>order(o(0),o(2))).toDF()
val orderItemsDF = sc.textFile("/user/cloudera/practice2/problem3/order_items").map(x=>x.split("\t")).map(i=>orderItems(i(0),i(5).toFloat)).toDF()

//joins on dataframes
val ordJoinDF = orderDF.join(orderItemsDF,"order_id").where("price > 200")

val ordCustDF = custDF.join(ordJoinDF,"customer_id").select("fname","lname","city","price")

ordCustDF.rdd.map(x => x.mkString(",")).saveAsTextFile("/user/cloudera/practice2/problem3/joinResults")

QUESTION 4 PREP
sqoop import \
--connect "jdbc:mysql://localhost/retail_db" \
--username root --password cloudera \
--table customers \
--warehouse-dir /user/cloudera/problem3/customers_hive/input \
--hive-import \
--create-hive-table \
--hive-database default \
--hive-table customers_hive

QUESTON 4
Instructions:

Get Customers from metastore table named "customers_hive" 
whose fname is like "Rich" and save the results in HDFS in text format.

Output Requirement:

Result should be saved in /user/cloudera/practice2/problem4/customers/output as text file. 
Output should contain only fname, lname and city
fname and lname should seperated by tab with city seperated by colon

Sample Output 
Richard Plaza:Francisco
Rich Smith:Chicago

val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
var result = hiveContext.sql("select concat(customer_fname,'\t',customer_lname,':',customer_city) from customers_hive where customer_fname like '%Rich%'")
result.rdd.map(x=>x.mkString(",".saveAsTextFile("user/cloudera/practice2/problem4/customers/output")))

QUESTION 5 PREP
sqoop import \
--connect "jdbc:mysql://localhost/retail_db" \
--password cloudera --username root \
--table customers  \
--target-dir /user/cloudera/problem2/customer/tab \
--fields-terminated-by "\t" \
--columns "customer_id,customer_fname,customer_state"

QUESTION 5
Provided tab delimited file, get total numbers customers in each state whose first name starts with 'M'
   and  save results in HDFS in json format.
Input folder
/user/cloudera/problem2/customer/tab
Result should be saved in /user/cloudera/problem2/customer_json_new.
Output should have state name followed by total number of customers in that state.

case class Customer(fname:String, state:String)
val tabFiles = sc.textFile("/user/cloudera/problem2/customer/tab").map(x=>x.split('\t')).map(c=>Customer(c(1),c(2))).toDF()
val groupData = tabFiles.where("fname like '%M%'").groupBy("state").count
groupData.write.json("/user/cloudera/problem2/customer_json_new")

QUESTION 6 PREP
sqoop import \
--connect "jdbc:mysql://localhost/retail_db" \
--username root --password cloudera \
--table products \
--warehouse-dir /user/cloudera/practice4.db \
--hive-import \
--create-hive-table \
--hive-database default \
--hive-table product_ranked -m 1

QUESTION 6
Provided  a meta-store table named product_ranked consisting of product details,
find the most expensive product in each category.
Output should have product_category_id ,product_name,product_price,rank.
Result should be saved in /user/cloudera/practice4/output/  as pipe delimited text file

var hc = new org.apache.spark.sql.hive.HiveContext(sc);
val hiveResult = hc.sql("select p.product_id, p.product_name, p.product_price, rank() over (partition by p.product_category_id) as product_price_rank from product_ranked p")
val expDF = hiveResult.filter("product_price_rank=1")
expDF.rdd.map(x=>x.mkString("|")).saveAsTextFile("/user/cloudera/practice4/output/")

QUESTION 7 PREP
sqoop import \
--connect "jdbc:mysql://localhost/retail_db" \
--password cloudera --username root \
--table orders \
--as-parquetfile \
--target-dir /user/cloudera/problem3/parquet

QUESTION 7
Fetch all pending orders from  data-files 
stored at hdfs location /user/cloudera/problem3/parquet 
and save it  into json file  in HDFS
Result should be saved in /user/cloudera/problem3/orders_pending
Output file should be saved as json file.
Output file should Gzip compressed.

var dataFile = sqlContext.read.parquet("/user/cloudera/problem3/parquet")
val filterData = dataFile.filter("order_status like "%PENDING%")
filterData.tojson.saveAsTextFile("/user/cloudera/problem3/orders_pending",classOf[org.apache.hadoop.io.compress.GzipCodec])

QUESTION 8 PREP 
sqoop import \
--connect "jdbc:mysql://localhost/retail_db" \
--password cloudera --username root \
--table customers    \
--columns "customer_id,customer_fname,customer_city"  \
--target-dir /user/cloudera/problem8/customer-avro \
--as-avrodatafile

QUESTION 8
Create a metastore table from avro files provided at below location.
Input folder is  /user/cloudera/problem8/customer-avro
Table name should be customer_parquet_avro
output location for hive data /user/cloudera/problem8/customer-parquet-hive
Output file should be saved in parquet format using GZIP compression.

HiveContext create table from avro files named customer_parquet_avro

import com.databricks.spark.avro._
val avroDF = sqlContext.read.avro("/user/cloudera/problem8/customer-avro")
sqlContext.setConf("spark.sql.parquet.compression.codec","gzip")
avroDF.write.parquet("/user/cloudera/problem8/customer-parquet-hive")


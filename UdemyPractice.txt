

####SQOOP ALL CUSTOMERS IN TX FIRST NAME MARY

QUESTION 1.
sqoop import \
--connect jdbc:mysql://quickstart:3306/retail_db \
--username=root --password=cloudera \
--table customers \
--target-dir /user/cloudera/problem1/customers/avrodata \
--compress
--compression-codec snappy \
--where "customer_state ='TX' and customer_fname='Mary'" \
--columns "customer_fname,customer_lname,customer_state" \
--as-avrodatafile

PREP FOR QUESTION 2
sqoop import \
--connect "jdbc:mysql://localhost/retail_db" \
--username root --password cloudera \
--table orders --target-dir /user/cloudera/practice1/question2  \
--fields-terminated-by "|" \
--columns "order_id,order_status"

sqoop eval \
--connect jdbc:mysql://quickstart:3306/retail_db \
--username=root --password=cloudera \
--query "create table retail_db.orders_exported(id int,status varchar(255))"

###LAUNCH MYSQL 
mysql -u root -p     exit

QUESTION 2.
sqoop export \
--connect "jdbc:mysql://localhost/retail_db" \
--username root --password cloudera \
--table orders_exported \
--export-dir "/user/cloudera/practice1/question2" \
--input-fields-terminated-by "|" 

PREP FOR QUESTION 3
sqoop import \
--connect "jdbc:mysql://localhost/retail_db" \
--password cloudera \
--username root \
--table orders \
--target-dir /user/cloudera/practice1/question3 \
--compress --compression-codec snappy --as-avrodatafile 

QUESTION 3 convert snappy compressed avro /user/cloudera/practice1/question3  into parquet /user/cloudera/practice1/question3/output/

####LAUNCH SPARK
spark-shell --master yarn-client

import com.databricks.spark.avro._
val dataFile = sqlContext.read.avro("/user/cloudera/practice1/question3")
sqlContext.setConf("spark.sql.parquet.compression.codec","gzip")
dataFile.select("order_id","order_status").write.parquet("/user/cloudera/practice1/question3/output/")

PREP FOR QUESTION 4 
sqoop import \
--connect "jdbc:mysql://localhost/retail_db" \
--username root --password cloudera \
--table orders --target-dir /user/cloudera/p1/p4/orders

sqoop import \
--connect "jdbc:mysql://localhost/retail_db" \
--username root --password cloudera \
--table customers --target-dir /user/cloudera/p1/p4/customers

QUESTION 4 join 2 file locations to find out  customers who have placed more than 4 orders
spark-shell --master yarn-client

case class cust(customer_id:String,customer_fname:String)
case class ord(customer_id:String,status:String)
val cusDF=sc.textFile("/user/cloudera/p1/p4/customers").map(x => x.split(",")).map(c => cust(c(0),c(1))).toDF()
val ordDF=sc.textFile("/user/cloudera/p1/p4/orders").map(x => x.split(",")).map(o => ord(o(2),o(3))).toDF()
ordDF.registerTempTable("orders")
val custOrd = sqlContext.sql("select customer_id, count(*) from orders group by customer_id having count(*)>4")
val joinDF = cusDF.join(custOrd,"customer_id")
joinDF.write.json("/user/cloudera/p1/p4/output")

PREP FOR QUESTION 5
sqoop import \
--connect "jdbc:mysql://localhost/retail_db" \
--password cloudera --username root \
--table products --target-dir /user/cloudera/practice1/problem5 \
--as-parquetfile \
--split-by "product_id"

QUESTION 5 move parquet files to another output folder

/user/cloudera/practice1/problem5 to /user/cloudera/practice1/problem5/output

Get maximum product_price in each product_category and order the results by maximum price descending.
Final output should have product_id and max_price separated by pipe delimiter
Ouput should be saved in text format with Gzip compression

spark-shell --master yarn-client

-group by category and grab max price and sort decending

var productDF = sqlContext.read.parquet("/user/cloudera/practice1/problem5");
productDF.registerTempTable("product")
val maxDF=sqlContext.sql("select product_category_id,max(product_price) from product group by product_category_id order by max(product_price) desc")
import org.apache.hadoop.io.compress.GzipCodec
maxDF.rdd.map(x => x.mkString("|")).saveAsTextFile("/user/cloudera/practice1/problem5/output",classOf[GzipCodec]);

QUESTION 6 PREP
sqoop import \
--connect "jdbc:mysql://localhost/retail_db" \
--password cloudera --username root \
--table customers \
--fields-terminated-by '\t' \
--columns "customer_id,customer_fname,customer_city"  \
--target-dir /user/cloudera/problem6/customer/text

QUESTION 6
Provided customer tab delimited files at below HDFS location.
Input folder is  /user/cloudera/practice1/problem6
Find all customers that lives 'Caguas' city.
Result should be saved in /user/cloudera/practice1/problem6/output
Output file should be saved in avro format in deflate compression.

spark-shell --master yarn-client
case class Customer(customer_id:String,name:String,city:String)
val custData = sc.textFile("/user/cloudera/practice1/problem6").map(x => x.split("\t")).map(c => Customer(c(0),c(1),c(2))).toDF()
val filteredData = custData.filter("city='Caguas'")
import com.databricks.spark.avro._
filteredData.write.avro("/user/cloudera/practice1/problem6/output2"); ?????????????

PREP QUESTION 7
sqoop import \
--connect "jdbc:mysql://localhost/retail_db" \
--password cloudera --username root \
--table customers  \
--target-dir /user/cloudera/practice1/problem7/customer/avro \
--columns "customer_id,customer_fname,customer_lname" \
--as-avrodatafile

QUESTION 7
Convert data-files stored at hdfs location /user/cloudera/practice1/problem7/customer/avro  
into tab delimited file using gzip compression and save in HDFS.
Result should be saved in /user/cloudera/practice1/problem7/customer_text_gzip 
Output file should be saved as tab delimited file in gzip Compression.
Sample Output:
21    Andrew   Smith
111    Mary    Jons

spark-shell --master yarn-client
import com.databricks.spark.avro._
val dataAvro = sqlContext.read.format("com.databricks.spark.avro").load("/user/cloudera/practice1/problem7/customer/avro") ###IN A DF
####MAP IT WITH \t in between each value and save as text files
dataAvro.map(x => x(0)+"\t"+x(1)+"\t"+x(2)).saveAsTextFile("user/cloudera/practice1/problem7/customer_text_gzip",classOf[org.apache.hadoop.io.compress.GzipCodec])

PREP FOR QUESTION 8
sqoop import \
--connect "jdbc:mysql://localhost/retail_db" \
--username root --password cloudera \
--table products  \
--hive-import \
--create-hive-table \
--hive-database default \
--hive-table product_replica -split-by product_id

QUESTION 8
Get products from metastore table named "product_replica" whose price > 100 
and save the results in HDFS in parquet format.
Result should be saved in /user/cloudera/practice1/problem8/product/output as parquet file
Files should be saved in Gzip compression.

val sqlContext = new org.apache.spark.sql.sqlContext(sc)
val result = sqlContext.sql("select * from product_replica where product_price>100")
result.parquet("/user/cloudera/practice1/problem8/product/output")

PREP QUESTION 9
sqoop import \
--connect "jdbc:mysql://localhost/retail_db" \
--password cloudera --username root \
--table customers \
--fields-terminated-by '\t' \
--columns "customer_id,customer_fname,customer_city"  \
--target-dir /user/cloudera/problem9/customer_text

QUESTION 9
Create a metastore table named customer_parquet_compressed from tab delimited files provided 
at below location.
Input folder is  /user/cloudera/problem9/customer-text
Schema for input  file

customer_id   customer_fname   customer_city

Use this location to store data for hive table: /user/cloudera/problem9/customer-hive
Output file should be saved in parquet format using Snappy compression.
For compression, below Hive property should be set to true
SET hive.exec.compress.output=true;

CREATE TABLE customer_parquet_compressed
STORE AS parquet
LOCATION '/user/cloudera/problem9/customer-text'
TBLPROPERTIES("parquet.compression"="SNAPPY")
AS SELECT * FROM customer_temp

################################

setConf between compressed and uncompressed
